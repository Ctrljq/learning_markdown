## 1、最小二乘原理

最小二乘原理（Least Squares Method）是一种用于数据拟合的数学方法，主要用于回归分析中，以最小化观测值与模型预测值之间的误差平方和。其基本思想是通过选择合适的模型，使得该模型预测值与实际观测值之间的差距最小。

### 最小二乘原理的基本步骤

1. **选择模型**：根据问题的性质选择一个合适的数学模型，通常是线性模型，如 \( y = ax + b \)。
  
2. **计算误差**：==对于每个观测值 \( $(x_i, y_i)$ \)，计算模型预测值与实际值之间的误差，即 \( $e_i = y_i - (ax_i + b)$ \)。==

3. **构建误差平方和**：==将所有误差的平方求和，构建目标函数：
   $
   S(a, b) = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - (ax_i + b))^2
   $==

4. **求解参数**：==通过对目标函数进行求导并设为零，求解出最优参数 \( a \) 和 \( b \)，使得误差平方和最小。==

### 应用实例

- **线性回归**：用于预测数据关系，如根据历史销售数据预测未来销售。
- **数据平滑**：在信号处理和图像处理中，最小二乘法可用于减少噪声影响，平滑曲线。
- **曲线拟合**：可以应用于科学实验数据的分析，以拟合复杂的非线性关系。

### 优势与劣势

| 优势                       | 劣势                       |
| -------------------------- | -------------------------- |
| 简单易懂，计算量较小       | 对异常值敏感，可能影响结果 |
| 可用于线性及某些非线性模型 | 可能导致欠拟合或过拟合     |
| 在多维空间中扩展性良好     | 假设数据误差是正态分布     |

最小二乘原理是回归分析中最常用的方法之一，在实际数据分析中发挥着重要作用。

## 2、k的推导

在最小二乘法中，我们通常需要推导出线性回归模型的参数，尤其是斜率 \( k \) 和截距 \( b \) 的公式。假设我们有 \( n \) 个观测点 \($ (x_i, y_i)$ \)，我们要拟合的线性模型为：

$
y = kx + b
$

### 推导步骤

1. **定义误差平方和**：
   误差平方和 \( S \) 定义为所有观测值 \( $y_i$ \) 与预测值 \( $\hat{y}_i $\) 的差的平方和：

   $
   S(k, b) = \sum_{i=1}^{n} (y_i - (kx_i + b))^2
   $

2. **对 \( S \) 关于 \( k \) 和 \( b \) 分别求偏导数**：
   我们希望找到使得 \( S \) 最小化的 \( k \) 和 \( b \)。为此，我们需要对 \( S \) 关于 \( k \) 和 \( b \) 分别求偏导数，并设为零。

   - ==对 \( b \) 的偏导数：==
   
   $
   \frac{\partial S}{\partial b} = -2\sum_{i=1}^{n} (y_i - (kx_i + b)) = 0
   $

   - ==对 \( k \) 的偏导数：==
   
   $
   \frac{\partial S}{\partial k} = -2\sum_{i=1}^{n} x_i (y_i - (kx_i + b)) = 0
   $

3. **解偏导数方程**：
   将偏导数方程整理，可以得到以下两个方程：

   $
   \sum_{i=1}^{n} y_i = nk + b\sum_{i=1}^{n} x_i
   $​
   
   
   
   $
   \sum_{i=1}^{n} x_iy_i = k\sum_{i=1}^{n} x_i^2 + b\sum_{i=1}^{n} x_i
   $​
   
4. **求解 \( k \) 和 \( b \)**：
   通过以上两个方程，我们可以求解出 \( k \) 和 \( b \)。

   - 首先可以用第一个方程求出 \( b \)：
   
   $
   b = \frac{\sum_{i=1}^{n} y_i - nk}{\sum_{i=1}^{n} x_i}
   $

   - 然后将 \( b \) 代入第二个方程中，进行整理，最后得到 \( k \) 的公式：

   $
   k = \frac{n\sum_{i=1}^{n} x_iy_i - \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{n\sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2}
   $

### 结论

通过以上推导，我们得到了线性回归模型中斜率 \( k \) 的公式，通常也会将其简化为：

==$
k = \frac{\text{Cov}(X, Y)}{\text{Var}(X)}
$==

==这里的 \($\text{Cov}(X, Y)$\) 表示 \(X\) 和 \(Y\) 的协方差，\($\text{Var}(X)$\) 表示 \(X\) 的方差。这个公式反映了变量之间的线性关系。==

## 3、ransac

![image-20241103144348465](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241103144348465.png)

下面将详细解释你提供的 RANSAC 算法步骤，以及每一步的含义和作用。

### RANSAC 算法步骤解析

1. **假设数据与模型**：
   - 假设我们有 \( P \) 个数据点 \(\{x_1, x_2, \ldots, x_P\}\)，并且我们要拟合一个由至少 \( n \) 个点决定的模型（对于圆，通常 \( n=3 \)，但如果是直线可以是 \( n=2 \)）。
   - 这个步骤的目的是明确需要多少个点来定义所需的模型。

2. **初始化迭代计数**：
   - 设置迭代计数 \( k = 1 \)。这表示我们将开始第一次迭代，并记录迭代次数。
   - 这一步的目的是为了控制算法的迭代次数，确保不会无限循环。

3. **随机选择点拟合模型**：
   - 从数据点 \( P \) 中随机选择 \( n \) 个点，并用这些点来拟合一个模型，记为 \( M_1 \)。
   - 这个步骤是 RANSAC 算法的关键部分，随机选择点的目的是为了提高算法的鲁棒性，使其不受异常值的影响。

4. **计算内点与重拟合**：
   - 给定容限误差 \( \epsilon \)，计算所有数据点 \(\{x_1, x_2, \ldots, x_P\}\) 中与模型 \( M_1 \) 的残差在容限 \( \epsilon \) 内的点的数量。
   - 如果内点的数量大于阈值 \( t \)，则认为拟合成功，可以根据内点集合 \( S_1 \) 重新拟合模型（通常使用最小二乘法或其变种）。
   - > 在 RANSAC 算法中，重新拟合模型是为了提高模型的准确性和鲁棒性。具体原因如下：
     >
     > ### 1. 利用更多的内点信息
     >
     > - **提高拟合质量**：最初的模型是基于随机选择的 \( n \) 个点拟合的，可能并不完全代表数据的整体趋势。通过重新拟合，我们可以利用从整个数据集中获得的内点集合 \( S_1 \)，这通常会提供更丰富的信息。
     >   
     > - **降低误差**：通过重新拟合，算法能够减少初始模型与实际数据之间的误差，因为内点集合通常包含更多符合模型假设的点。
     >
     > ### 2. 增强模型的稳定性
     >
     > - **抑制噪声影响**：初始模型可能受到随机选择的噪声数据的影响，而通过重新拟合内点，模型更有可能反映出真实的趋势，而不是噪声或异常值的影响。
     >
     > - **增加鲁棒性**：在数据集中，噪声和异常值是常见的。重新拟合后，模型会对这些不符合的数据点更加鲁棒，增强其适用性。
     >
     > ### 3. 最小二乘法的应用
     >
     > - **使用最小二乘法**：重新拟合模型通常会使用最小二乘法或其变种，这种方法可以最小化内点与模型之间的残差平方和，从而得到一个更优的拟合模型。
     >
     > ### 4. 改进算法输出
     >
     > - **提高最终结果的准确性**：最终输出的模型是基于内点集合重新拟合得出的，确保模型能够更好地预测未见数据或新的数据点，提升了算法的实际应用效果。
     >
     > ### 总结
     >
     > 重新拟合模型是 RANSAC 算法中的重要步骤，旨在利用内点集合的信息来获得一个更准确、更稳定的模型。这不仅提高了模型的拟合质量，还增强了其在实际应用中的鲁棒性和有效性。通过这一过程，算法能够在处理具有噪声和异常值的数据时，更加有效地提取出有用的模式和结构。
   - 这一步的目的是通过检验内点数量来评估模型的质量，如果满足条件，意味着当前模型能够很好地拟合大部分数据点。
   
5. **迭代与终止条件**：
   - 增加迭代计数 \( k \)（即 \( k = k + 1 \)）。如果当前的 \( k \) 小于预设的最大迭代次数 \( K \)，则返回第 3 步继续下一次迭代。
   - 如果 \( k \) 达到或超过 \( K \)，则选择当前内点最多的模型作为最终模型，或者判定算法失败。
   - 这一步确保算法在一定次数内能尝试多次随机选择，增加找到合适模型的概率。
   - > ### 处理内点数量小于阈值 ttt 的情况
     >
     > 1. **忽略当前模型**：
     >    - 如果内点数量小于 ttt，算法会放弃当前模型 M1M_1M1 的结果，认为它不够可靠。
     > 2. **继续迭代**：
     >    - 迭代计数 kkk 将增加（例如，执行 k=k+1k = k + 1k=k+1），然后返回到步骤 3，进行下一次迭代。
     >    - 在下一次迭代中，算法将再次从数据点中随机选择 nnn 个点，拟合一个新的模型 M1M_1M1。
     > 3. **重复过程**：
     >    - 这个过程将重复进行，直到达到预设的最大迭代次数 KKK。如果在 ( K \ 次迭代中仍未找到足够内点的模型，则算法可能会终止。
     > 4. **终止与输出**：
     >    - 如果在达到最大迭代次数后仍没有一个有效的模型（即没有一个模型的内点数量超过 ttt），算法会返回一个失败状态，或者输出当前内点最多的模型（如果有的话）。
     >    - 有时候，算法也可以输出最终找到的最佳模型，这个模型是内点数量最多的模型。
     >
     > ### 总结
     >
     > 当内点数量小于阈值 ttt 时，算法会放弃当前模型，继续寻找更好的模型。这个设计保证了 RANSAC 的鲁棒性，使其能够在含有异常值的情况下仍然找到合适的拟合模型。通过多次随机选择和模型评估，最终的输出将是一个尽可能准确且可靠的模型。

### 总结

RANSAC 算法的核心思想是通过随机抽样和内点判断来拟合模型，以应对数据中的异常值和噪声。每次迭代的随机选择和内点的重新评估确保了算法的鲁棒性和灵活性。通过设定最大迭代次数 \( K \) 来避免无限循环，确保算法在合理的时间内返回结果。最终，选择内点最多的模型作为输出结果，这样能够保证拟合的质量和有效性。

## 4、容限误差

![image-20241103145601504](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241103145601504.png)

## 5、迭代次数k

![image-20241103145630424](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241103145630424.png)

> $\omega$（即从  P 个点中随机选择的一点符合容限误差的概率）是一个重要的参数。以下是如何确定 $\omega$ 的一些方法和考虑因素：
>
> ### 1. 数据集的特性
>
> - **先验知识**：如果对数据集有先验了解，例如数据点的分布特性、噪声水平等，可以根据这些信息估计 $\omega$ 。例如，如果知道数据中大部分点都符合模型，则可以设定一个较高的 $\omega$ 值。
>
> ### 2. 统计分析
>
> - **实验性估计**：可以通过初步分析数据集，随机抽样一定数量的点，计算这些点中符合容限误差的点的比例。这可以提供一个经验值作为 $\omega$ 。
>
> ### 3. 实验和调整
>
> - **动态调整**：在算法运行过程中，可以通过观察内点的数量变化，动态调整 $\omega$ 。例如，如果发现随机选取的点大多数都不符合容限误差，可以适当降低 $\omega$ 。
>
> ### 4. 确定性选择
>
> - **明确阈值**：对于特定的应用场景，可能已经有研究或文献提供了相应的数据特征，可以直接参考这些确定的值。

> ## E(k)推导
>
> 该公式的推导基于几何级数和其求导性质。我们来看具体推导步骤。
>
> 1. **定义和解释**：
>    - \( E(k) \) 是期望值，即找到一个有效模型所需的平均迭代次数。
>    - \( b \) 表示一次随机选择 \( n \) 个点且这些点都符合模型的概率，给出 \( b =$ \omega^n $\)。
>    - \( a = 1 - b \)，表示一次随机选择 \( n \) 个点且至少有一个点不符合模型的概率。
>
> 2. **期望的推导**：
>    - 从公式的第一个等式我们可以看到，\( E(k) \) 的形式是一个带有 \( b \) 的几何级数。级数的每一项表示在第 \( i \) 次尝试时成功的概率和期望值。
>    - 期望值 \( E(k) \) 的展开为：
>      $
>      E(k) = b + 2 \cdot (1 - b) \cdot b + 3 \cdot (1 - b)^2 \cdot b + \ldots
>      $
>    - 这代表了多次尝试的期望，每一项的系数 \( i \) 表示第 \( i \) 次尝试成功的期望次数，而 \( $(1 - b)^{i-1} \cdot b$ \) 是在前 \( i-1 \) 次尝试失败，第 \( i \) 次尝试成功的概率。
>
> 3. **几何级数求和公式**：
>    - 为了求出 \( E(k) \) 的和，我们利用几何级数求和公式：
>      $
>      \sum_{i=0}^{\infty} x^i = \frac{1}{1 - x}
>      $
>    - 该公式的求导（对 \( x \) 求导）可以得到一个带系数 \( i \) 的几何级数求和形式：
>      $
>      \sum_{i=1}^{\infty} i \cdot x^{i-1} = \frac{1}{(1 - x)^2}
>      $
>
> 4. **将几何级数公式应用到期望值计算中**：
>    - 设 \( x = 1 - b \)，那么有
>      $
>      E(k) = \frac{1}{b}
>      $
>    - 从而得到 \($ E(k) = \frac{1}{b} = \frac{1}{\omega^n} $\)。
>
> ### 最终结果
>
> 根据以上推导，我们得到了结果：
> $
> E(k) = \frac{1}{b} = \omega^{-n}
> $

![image-20241103150730552](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241103150730552.png)

## 6、阈值t

![image-20241103150821896](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241103150821896.png)