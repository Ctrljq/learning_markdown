## PAF标签

> 这里的 **"2C"** 是指每个 **连接对** 的信息在标签中占用两个通道，分别对应**x方向**和**y方向**的分量。让我们详细拆解一下这个概念。
>
> ---
>
> ### **背景：PAF是向量场**
> PAF表示的是人体中两个关键点之间的方向信息。假设我们有两个关键点 \( A \) 和 \( B \)，分别位于图像中的坐标：
> - \( A = (x_A, y_A) \)
> - \( B = (x_B, y_B) \)
>
> 那么，连接 \( A \) 和 \( B \) 的向量方向可以表示为：
> \[
> \vec{v} = (v_x, v_y) = \left(\frac{x_B - x_A}{d}, \frac{y_B - y_A}{d}\right)
> \]
> 其中 \( d = \sqrt{(x_B - x_A)^2 + (y_B - y_A)^2} \) 是 \( A \) 和 \( B \) 之间的距离，用来归一化这个向量，使其成为单位向量。
>
> 这个单位向量 \( \vec{v} \) 的两个分量：
> - \( v_x \)：x方向上的分量。
> - \( v_y \)：y方向上的分量。
>
> ---
>
> ### **2C的意义**
> 如果人体骨架中有 \( C \) 个关键点连接对（例如，左肩-左肘、左肘-左手腕等），那么对于每个连接对：
> - 用一个二维向量 \( (v_x, v_y) \) 表示连接方向。
> - 为了存储这个向量的信息，我们需要两个通道：
>   - **第一个通道**存储 \( v_x \)（x方向分量）。
>   - **第二个通道**存储 \( v_y \)（y方向分量）。
>
> 因此，整个PAF标签的维度是 \( H \times W \times 2C \)：
> - \( H, W \)：图像的高度和宽度。
> - \( 2C \)：对于每个连接对，分配两个通道，分别存储x方向和y方向的分量。
>
> ---
>
> ### **具体的标签生成过程**
> 对于每个连接对（比如左肩和左肘）：
> 1. **标注两个关键点的位置 \( A, B \)。**
> 2. **计算单位向量 \( \vec{v} = (v_x, v_y) \)。**
> 3. **将这个向量分布到图像上，覆盖关键点之间的像素区域。**
>    - 在连接两个关键点的区域中：
>      - 第一个通道存储 \( v_x \)。
>      - 第二个通道存储 \( v_y \)。
>    - 其他区域的值为零。
>
> 最终，所有连接对的信息会叠加到一个 \( H \times W \times 2C \) 的张量中。
>
> ---
>
> ### **案例说明**
> 假设我们有以下3个关键点连接对（左肩-左肘、左肘-左手腕、右肩-右肘），即 \( C = 3 \)：
> - 第1对：左肩 \( A_1 \) 到左肘 \( B_1 \)。
> - 第2对：左肘 \( A_2 \) 到左手腕 \( B_2 \)。
> - 第3对：右肩 \( A_3 \) 到右肘 \( B_3 \)。
>
> 那么，PAF的标签维度为：
> \[
> H \times W \times 2C = H \times W \times 6
> \]
> 对应的6个通道：
> 1. 通道1：左肩到左肘的 \( v_x \) 分量。
> 2. 通道2：左肩到左肘的 \( v_y \) 分量。
> 3. 通道3：左肘到左手腕的 \( v_x \) 分量。
> 4. 通道4：左肘到左手腕的 \( v_y \) 分量。
> 5. 通道5：右肩到右肘的 \( v_x \) 分量。
> 6. 通道6：右肩到右肘的 \( v_y \) 分量。
>
> ---
>
> ### **总结**
> - **2C**：每个连接对需要2个通道分别存储 \( x \) 和 \( y \) 方向的分量。
> - **作用**：用这2个通道描述了关键点对之间的方向关系，帮助模型在训练时学习如何推断人体骨架结构。
>
> 如果还有疑问，可以结合具体代码或实例进一步讨论！

> 是的，**不在关键点连接的区域内的区域**，对应的 \( H \times W \times 2C \) 中的值通常被设置为 0。这是因为在这些区域中没有相关的关键点连接信息，向量场并不适用。下面我们详细解析这个过程。
>
> ---
>
> ### **1. 区域划分**
> 对于每对关键点的PAF标签：
> - **连接区域（连接两个关键点的线段周围区域）**：
>   - 在这部分区域内，PAF会存储从一个关键点到另一个关键点的方向向量 \( \vec{v} = (v_x, v_y) \)。
> - **非连接区域（其他像素点）**：
>   - 在这部分区域内，没有对应的关键点连接，因此这部分PAF值被设置为0，表示没有向量。
>
> ---
>
> ### **2. 为什么只在连接区域设置向量值？**
> 这是因为PAF标签的目的是学习人体骨架中关键点对的连接关系。模型只需要在关键点连接区域内学习到正确的方向向量，而其他区域的数据对训练无帮助。
>
> 为了避免噪声和影响模型的收敛，非连接区域被设置为 0，标识这些区域不包含有意义的信息。
>
> ---
>
> ### **3. 如何定义连接区域？**
> 在实际生成标签时，连接区域通常定义为连接两个关键点的线段加上一定宽度的带状区域。例如：
> - **线段**：从关键点 \( A \) 到 \( B \) 的直线。
> - **带状区域**：在线段的基础上，向两侧扩展一定的宽度（通常用固定的像素距离定义）。
>
> 在这个带状区域内，PAF的值为该连接的方向向量 \( \vec{v} \)。区域外的值设置为 0。
>
> ---
>
> ### **4. 标签生成中的细节**
> 对于每个连接对：
> 1. 计算从关键点 \( A \) 到关键点 \( B \) 的单位向量 \( \vec{v} = (v_x, v_y) \)。
> 2. 在连接区域内的每个像素点：
>    - 分别在PAF的两个通道中设置 \( v_x \) 和 \( v_y \)。
> 3. 在非连接区域内的像素点：
>    - 对应的两个通道设置为 0。
>
> 最终，整个PAF标签对于某个连接对看起来像这样：
> - **连接区域**：填充 \( v_x \) 和 \( v_y \)。
> - **其他区域**：填充 0。
>
> ---
>
> ### **5. 标签中设置为 0 的区域意义**
> - **背景区域**：即图像中没有关键点的部分。PAF在这些区域上是完全无关的，因此直接设置为 0。
> - **非连接区域**：两个关键点之间的连接不涉及的部分，比如肩膀和肘部之间的PAF不应干扰手腕和膝盖之间的区域。
> - **避免错误学习**：设置为 0 的区域可以防止模型在无意义的区域内学习错误的方向。
>
> ---
>
> ### **6. 示例说明**
> 假设图像大小为 \( H \times W = 100 \times 100 \)，有一个关键点连接对：左肩 \( A \) 和左肘 \( B \)。以下是标签生成的结果：
> - 在从 \( A \) 到 \( B \) 的连接区域（如一个直线+带状区域内），PAF的两个通道存储：
>   - 通道1（x方向分量）：存储单位向量的 \( v_x \)。
>   - 通道2（y方向分量）：存储单位向量的 \( v_y \)。
> - 在其他所有像素区域（包括背景区域和不相关部分），PAF的两个通道都设置为 0。
>
> 最终，整个标签在不相关区域的值为 0，而关键点连接区域内存储了方向信息。
>
> ---
>
> ### **总结**
> 是的，在不属于关键点连接区域的像素位置，PAF的值被设置为 0。这种设计可以有效帮助模型集中学习关键点之间的连接信息，同时忽略不相关的区域，提升训练效率和模型的表现。

## 关键点检测中，为什么要生成高斯图，而不是直接与ground_truth比较

[(13 封私信) 关键点检测中，为什么要生成高斯图，而不是直接与ground_truth比较？ - 知乎](https://www.zhihu.com/question/293815527)

> 很多任务中，目标点其实很难准确的被某一个像素位置定义的，也就很难被准确的标注。目标点附近的点其实也很像目标点，我们直接将其标为负样本，可能给网络的训练带来干扰，将其用[高斯函数](https://zhida.zhihu.com/search?content_id=148274445&content_type=Answer&match_order=1&q=高斯函数&zhida_source=entity)做一个**“软标注**”，网络也就更好收敛。
>
> 加上[高斯图](https://zhida.zhihu.com/search?content_id=148274445&content_type=Answer&match_order=2&q=高斯图&zhida_source=entity)，也能够给网络的训练增加一个方向性的引导，距离目标点越近，激活值越大，这样网络能有方向的去快速到达目标点。

> 使用heatmap回归相对于直接回归关键点坐标能够显式地“对非关键点的响应进行抑制”。试想一下场景（下图），对于某一关键点，特征图有两个位置都做出响应，一个tp（true positive），一个fp（false positive），回归heatmap可以显式地抑制fp；而[回归坐标](https://zhida.zhihu.com/search?content_id=146172616&content_type=Answer&match_order=1&q=回归坐标&zhida_source=entity)只是一个隐式的抑制过程，可能tp处响应较大就忽略了其他fp，最后loss也比较小，但在inference时，数据分布的不同可能导致fp的响应超过了tp，出现预测错误，这也是直接回归坐标点的模型泛化能力差的原因之一

## anns

```json
{'segmentation': [[427.67, 203.36, 403.88, 203.65, 405.03, 190.18, 406.75, 183.59, 411.33, 181.58, 410.47, 176.14, 410.76, 170.12, 418.5, 170.69, 422.22, 175.28, 422.22, 178.43, 427.95, 181.58, 431.68, 190.75, 431.11, 198.78]], 'num_keypoints': 0, 'area': 675.29595, 'iscrowd': 0, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'image_id': 204805, 'bbox': [403.88, 170.12, 27.8, 33.53], 'category_id': 1, 'id': 1228478}
```

### 

| 字段名          | 类型       | 说明                                                         |
| --------------- | ---------- | ------------------------------------------------------------ |
| `segmentation`  | List[List] | **分割信息**：表示目标的分割轮廓，多边形点的坐标。通常用于语义分割或实例分割任务。  例如，`[x1, y1, x2, y2, ...]`，是一组多边形的顶点坐标。 |
| `num_keypoints` | Integer    | **关键点数量**：当前标注中标注的关键点总数。 例如，`0` 表示当前对象没有关键点标注。 |
| `area`          | Float      | **目标面积**：该目标占用的像素面积，用于衡量目标的大小。 例如，`675.29595` 表示目标的面积为 675.30 个像素。 |
| `iscrowd`       | Integer    | **是否拥挤（IsCrowd）**：表示目标是否是一个“拥挤区域”。 `0` 表示不是拥挤目标（正常对象），`1` 表示是拥挤目标（例如人群）。 |
| `keypoints`     | List       | **关键点信息**：关键点坐标和可见性标志，每 3 个值描述一个关键点：`[x, y, v]`。 其中，`x, y` 是坐标，`v` 是可见性： - `0`：不可见 - `1`：图像中不可见但标注 - `2`：可见  例如，这里全为 `0`，表示无关键点。 |
| `image_id`      | Integer    | **图像 ID**：唯一标识一张图像的 ID。这里的 `204805` 对应 COCO 数据集中的一张图片。 |
| `bbox`          | List       | **边界框（Bounding Box）**：目标的外接矩形框，用 `[x_min, y_min, width, height]` 表示。 例如，`[403.88, 170.12, 27.8, 33.53]` 表示： 左上角坐标为 `(403.88, 170.12)`，宽度为 `27.8`，高度为 `33.53`。 |
| `category_id`   | Integer    | **类别 ID**：目标所属的类别编号。这里 `1` 表示某一类别，例如“人体”。 |
| `id`            | Integer    | **标注 ID**：唯一标识该标注的 ID。例如，`1228478` 对应 COCO 数据集中的某个标注实例。 |

------

## metadata

```json
{'offset': array([-25.,  54.]), 'scale': array([0.66041667, 0.659375  ]), 'valid_area': array([ 25.,   0., 317., 368.]), 'hflip': True, 'width_height': array([480, 640]), 'horizontal_swap': <function horizontal_swap_coco at 0x000002187EF3CB80>, 'dataset_index': 10, 'image_id': 574769, 'file_name': '000000574769.jpg'}
```

```
{
    'hflip': False,                 # 水平翻转标志
    'offset': [ -9., -67.],         # 图像偏移量
    'scale': [0.7, 0.7005988],      # 图像缩放比例
    'valid_area': [  9.,  67., 350., 234.],  # 有效区域
    'width_height': [500, 334]      # 图像宽度和高度
}

```

## 输入(368,368)的图片会怎么处理

```
Compose(
    ColorJitter(brightness=(0.9, 1.1), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.1, 0.1))
    RandomApply(
    p=0.1
    Lambda()
)
    RandomGrayscale(p=0.01)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
```

> 这段代码展示了一个通过 `torchvision.transforms.Compose` 组合起来的图像数据增强和预处理流水线，主要对输入图片进行颜色调整、随机灰度化、归一化等操作。以下是对每个步骤的详细解析以及对 `(368, 368)` 尺寸图片的具体处理过程。
>
> ---
>
> ### **流水线中的各个步骤**
>
> 1. **`ColorJitter`**:
>    ```python
>    ColorJitter(brightness=(0.9, 1.1), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.1, 0.1))
>    ```
>    - **功能**: 随机调整图片的亮度、对比度、饱和度和色调。
>    - **参数**:
>      - `brightness=(0.9, 1.1)`：亮度在原值的 90% 到 110% 之间随机变化。
>      - `contrast=(0.9, 1.1)`：对比度在原值的 90% 到 110% 之间随机变化。
>      - `saturation=(0.9, 1.1)`：饱和度在原值的 90% 到 110% 之间随机变化。
>      - `hue=(-0.1, 0.1)`：色调在 -0.1 到 0.1 之间随机变化（单位为色环的一部分，值越小变化越微弱）。
>    - **作用**: 改变图片颜色属性，增加数据多样性。
>    - **影响**: 图片尺寸不变，仍为 `(368, 368)`。
>
> ---
>
> 2. **`RandomApply`**:
>    ```python
>    RandomApply([Lambda()], p=0.1)
>    ```
>    - **功能**: 以概率 `p=0.1`（10% 的概率）随机执行 `Lambda` 内的操作。
>    - **`Lambda()` 的作用未知**:
>      - 如果 `Lambda()` 中定义了图片变换（如自定义的裁剪、变换），会进一步改变图片，但目前未明确。
>    - **作用**: 提供额外的自定义操作灵活性。
>    - **影响**: 如果不触发（90% 的情况），图片不变；如果触发，变化取决于 `Lambda` 的定义。
>
> ---
>
> 3. **`RandomGrayscale`**:
>    ```python
>    RandomGrayscale(p=0.01)
>    ```
>    - **功能**: 以概率 `p=0.01`（1% 的概率）将图片转换为灰度图（单通道），然后再将其扩展为 3 通道以保持维度一致。
>    - **作用**: 模拟黑白图像，增加模型对灰度信息的鲁棒性。
>    - **影响**:
>      - 99% 的情况下，图片保持原有 RGB 通道；
>      - 1% 的情况下，图片变成灰度后扩展回 RGB 通道，尺寸仍为 `(368, 368, 3)`。
>
> ---
>
> 4. **`ToTensor`**:
>    ```python
>    ToTensor()
>    ```
>    - **功能**: 将图片从 PIL 格式或 NumPy 数组格式转换为 PyTorch 的张量格式，并将像素值从 `[0, 255]` 归一化为 `[0, 1]`。
>    - **作用**: 准备输入数据，使其适应深度学习框架。
>    - **影响**:
>      - 图片尺寸从 `(368, 368, 3)` 转换为 `(3, 368, 368)`（通道维度前置）。
>      - 像素值缩放至 `[0, 1]`。
>
> ---
>
> 5. **`Normalize`**:
>    ```python
>    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
>    ```
>    - **功能**: 对图片通道进行归一化处理：
>      - 对每个通道的像素值执行以下公式：
>        \[
>        \text{normalized\_pixel} = \frac{\text{original\_pixel} - \text{mean}}{\text{std}}
>        \]
>        其中，`mean` 和 `std` 是 ImageNet 数据集的统计值。
>      - `mean=[0.485, 0.456, 0.406]` 对应 RGB 三个通道的均值。
>      - `std=[0.229, 0.224, 0.225]` 对应 RGB 三个通道的标准差。
>    - **作用**: 将图像标准化为零均值和单位方差，以加速模型收敛。
>    - **影响**:
>      - 图像数据范围从 `[0, 1]` 转换为归一化后的范围。
>      - 尺寸仍为 `(3, 368, 368)`。
>
> ---
>
> ### **整体处理流程总结**
> 1. **输入图片**:
>    - 原始图片尺寸为 `(368, 368, 3)`，像素值在 `[0, 255]`。
>
> 2. **处理后输出图片**:
>    - 输出尺寸为 `(3, 368, 368)`，经过颜色调整、可能的灰度化、归一化后，像素值范围根据 `Normalize` 转换为小范围浮点值（取决于输入像素值和归一化参数）。
>
> ---
>
> 
>

## 网格对齐

> ![img](https://pica.zhimg.com/v2-719d4477f6bbf0a4ec5a3e690e42d418_1440w.jpg)
>
> ![img](https://pica.zhimg.com/v2-70b12ba49b69ffcc1475891c90f0a8e2_1440w.jpg)
>
> `start = stride / 2.0 - 0.5` 的目的在于计算热图网格的 **起始偏移量**，它确保了网格中心点的正确定位。下面逐步解释为什么要加 `start`：
>
> ### 1. **网格坐标和图像坐标系的区别**
>    在计算机视觉中，图像的坐标系通常以 **左上角为原点**，并且是基于 **像素的左上角** 来定位的。而热图上的每个点的坐标通常是基于 **网格中心** 来定义的（如在特征图中），因此需要将网格点坐标转换成对应的图像坐标。
>
> ### 2. **`stride`的作用**
>    `stride` 是每个网格的大小，即每个网格在图像空间中对应的像素范围。例如，如果 `stride = 8`，则每个网格在图像中占据 8x8 个像素。`stride` 确定了网格在图像中分布的密度。
>
>    - 如果 `stride = 8`，这意味着热图的每个网格代表图像中的 8x8 个像素。
>    - 在热图中，网格通常位于像素的 **中心**，而在图像中每个网格的范围是基于像素的 **左上角**。
>
> ### 3. **`start` 的计算**
>    `start = stride / 2.0 - 0.5` 用于确定网格中心点在图像坐标中的 **实际偏移**，以便将网格的中心点正确对齐到图像坐标系中。
>
>    - `stride / 2.0`：是每个网格的中心点距离网格左上角的距离。例如，如果 `stride = 8`，则网格中心离左上角的距离是 4。
>    - `-0.5`：是因为 **图像坐标** 系统的单位是像素，而 **网格坐标** 系统的单位是网格。为了保证正确对齐，中心点需要向左和向上偏移一点。
>
>    这样计算 `start` 是为了让网格的中心点准确地映射到图像坐标系中。
>
> ### 4. **为什么要加 `start`**
>    当你将网格坐标（如热图上的 `(0, 0)`, `(1, 0)`, `(2, 0)`）转换为图像坐标时，每个网格的实际中心点应该与图像的像素对应。通过加上 `start`，你可以确保网格中心在图像中的位置正确对齐，而不是出现在网格左上角的位置。
>
>    举个例子，如果 `stride = 8`，那么每个网格的中心应该位于每 8 个像素的中间。`start` 确保了这一点，使得每个热图网格的中心正确地与图像中的对应区域对齐。
>
> ### 5. **代码解析**
>    ```python
>    xx = xx * stride + start  # 调整网格的 x 坐标到图像中的实际坐标
>    yy = yy * stride + start  # 调整网格的 y 坐标到图像中的实际坐标
>    ```
>
>    在这两行代码中：
>    - `xx * stride` 和 `yy * stride` 将网格坐标乘以 `stride`，使其变为图像空间中的像素级别。
>    - `+ start` 将坐标偏移，使网格的中心对齐到图像坐标中。
>
> ### 6. **总结**
>    `start` 用来对每个网格进行精确的偏移调整，确保网格的中心点正确地映射到图像的实际坐标。这样做的目的是为了避免网格坐标的左上角与图像像素坐标的不对齐问题，保证热图与图像的准确匹配。