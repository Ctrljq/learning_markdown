# 一、前置知识

## 1、BEV&3D重建

BEVFormer（Bird's Eye View Transformer）中的重构特征空间确实有点类似于3D重建，但与传统的3D重建有显著区别。

> ### BEVFormer的特征空间解析：
>
> 1. **特征空间构建**：
>
>    - BEVFormer主要通过Transformer机制将多视角的2D特征映射到鸟瞰视角（BEV）的特征空间。
>    - 这种特征空间是**BEV格式**的，它并不直接包含显式的3D几何信息，而是通过多视角信息融合和位置编码间接建模场景的3D关系。
>
> 2. **与传统3D重建的对比**：
>
>    | **特性**       | **BEVFormer**                               | **传统3D重建**                           |
>    | -------------- | ------------------------------------------- | ---------------------------------------- |
>    | **输入**       | 多视角2D图像特征                            | 多视角图像或点云                         |
>    | **输出**       | BEV特征图，用于下游任务（如检测、分割）     | 3D点云或网格模型，直接重建场景的几何结构 |
>    | **表示方式**   | 隐式3D表征，通过BEV特征表示                 | 显式3D表征（例如点、网格）               |
>    | **目标**       | 融合多视角信息，提高BEV上的检测、分割等性能 | 精确还原场景的几何和纹理信息             |
>    | **计算复杂度** | 较低，依赖Transformer和位置编码优化         | 较高，依赖稀疏点云处理或密集几何计算     |
>
> 3. **场景关系建模**： BEVFormer利用Transformer机制：
>
>    - 编码器部分：通过多头注意力将多视角特征整合到一个全局一致的BEV特征空间中。
>    - 解码器部分：基于参考点将BEV特征与多视角特征匹配，完成动态更新。
>    - 最终生成的BEV特征空间是一种结构化表征，保留了3D信息的核心内容，但没有直接显式重建三维形状。
>
> 4. **应用与优势**： BEV特征空间更注重功能性：
>
>    - 用于目标检测、分割、行为预测等任务。
>    - 相较于传统3D重建，它忽略了重建几何细节，专注于语义层面的理解，因此更高效且更适合下游任务。
>
> ### 简单结论：
>
> BEVFormer重构的特征空间并非传统意义上的3D重建结果，但可以视为一种语义化的隐式3D表征。它利用多视角融合生成的BEV特征空间，在特定任务上提供了类似3D重建的功能效果，但专注于任务需求而非几何精度。
>
> <video src="C:/Users/HUAWEI/Desktop/Photo%20Library/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/QQ20241211-145630.mp4"></video>
>
> **BEV特征空间**和**SIFT特征点匹配拼接**在目标、方法和结果上都有显著差异。以下是它们的对比分析：

------

## 2、**核心区别：BEV特征空间 vs SIFT特征点拼接**

| **方面**             | **BEV特征空间**                                              | **SIFT特征点拼接**                                           |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **目标**             | 提取多视角特征，生成鸟瞰视角(BEV)的语义化隐式特征空间，用于检测或分割等任务 | 通过特征点匹配和拼接生成多视角的全景图或重建几何关系         |
| **输入**             | 多帧图像的2D特征（通过CNN提取）和摄像机位置信息              | 原始图像序列，无需额外的摄像机位置信息                       |
| **输出**             | BEV格式的特征图，间接表征场景的3D语义关系                    | 图像或特征的几何对齐结果，例如拼接后的全景图或对应的匹配关系 |
| **方法**             | 使用Transformer结合位置编码、多视角信息融合生成特征空间      | 基于SIFT算法提取关键点后计算特征匹配并通过变换矩阵拼接图像   |
| **几何建模**         | 隐式建模3D关系，通过参考点与视图变换间接学习                 | 显式建模特征点间的几何变换（例如单应性矩阵或基础矩阵）       |
| **是否依赖相机参数** | 是，使用摄像机内外参构建视角转换                             | 否，仅基于图像本身的特征点                                   |
| **适用场景**         | 自动驾驶、3D目标检测、语义分割                               | 图像拼接、全景图生成、稀疏3D点云恢复                         |
| **计算复杂度**       | 较高，依赖深度学习网络进行多视角融合                         | 较低，基于局部特征点的经典算法实现                           |
| **结果特性**         | 高语义，生成的BEV特征空间用于进一步任务（检测、预测等）      | 高几何精度，拼接的图像精确对齐但语义信息较弱                 |

------

## 3、**深入分析：BEV特征空间 vs SIFT匹配结果**

> #### **1. 特征提取方法**：
>
> - BEV特征空间：
>
>   - 特征由深度学习网络（例如ResNet或Transformer编码器）从图像中提取，包含**全局语义信息**，例如场景物体的类别、位置和关系。
>   - 特征经过多视角融合和位置编码后，映射到鸟瞰视角，形成高维BEV特征空间。
>   
> - SIFT特征点拼接：
>
>   - 特征点是局部的，基于梯度方向的变化提取。主要用于图像配准和几何对齐，不包含显式语义信息。
> - 通过特征点匹配计算变换矩阵（如单应性矩阵），实现图像的空间拼接。
>
> #### **2. 几何建模方式**：
>
> - BEV特征空间：
>
>   - 通过摄像机位姿和参考点的动态交互，隐式学习3D关系。
>   - 结果是鸟瞰视角的特征表征，不追求精确的几何对齐，但能有效表示场景布局。
>   
> - SIFT特征点拼接：
>
>   - 直接基于特征点匹配，使用几何约束（如RANSAC）计算变换。
> - 输出结果几何对齐精确，但无法提供全局场景的语义信息。
>
> #### **3. 输出的应用场景**：
>
> - **BEV特征空间**：
>   - 应用于自动驾驶、目标检测、语义分割等任务。
>   - 结果更关注全局信息和任务相关性，而不是图像的几何精度。
> - **SIFT特征点拼接**：
>   - 应用于图像拼接、稀疏3D点云重建等场景。
>   - 结果更注重局部几何对齐精度，但缺乏高层次语义。
>
> ------
>
> ### **实际工作中的选择**：
>
> 1. 如果目标是**多视角信息融合**以进行目标检测或语义分析（如自动驾驶、无人机导航）：
>    - **选择BEV特征空间**，因为它提供了结构化的3D信息，且能够整合更多的语义内容。
> 2. 如果目标是**图像对齐**或**生成全景图**（如街景拼接、增强现实）：
>    - **选择SIFT特征点拼接**，因其在几何对齐上效率高且精度好。
>
> ------
>
> ### **总结**：
>
> BEV特征空间关注语义化的场景表征，用于下游智能任务；SIFT特征点匹配则侧重几何对齐，适合图像级的拼接或对齐任务。两者在目标和实现方式上完全不同，且适用场景差异显著。
>
> > 是的，**BEVFormer**和**SIFT拼接**在视角和结果上有本质的区别：
> >
> > ------
> >
> > ### **BEVFormer**生成的鸟瞰图：
> >
> > 1. 视角
> >
> >    ：
> >
> >    - BEVFormer通过摄像机的内外参信息，将多视角图像的2D特征转换到**上帝视角（Bird's Eye View, BEV）**，即俯视的鸟瞰图。
> >    - 这种视角不再依赖具体的拍摄相机视角，而是对场景进行了3D信息的语义融合和转换。
> >
> > 2. 结果
> >
> >    ：
> >
> >    - 输出的是**BEV特征图**，用于检测、分割、预测等任务。
> >    - 它并非一张可视化的图像，而是高维特征空间表征，可以通过后处理生成与鸟瞰图类似的效果图。
> >
> > ------
> >
> > ### **SIFT拼接**得到的全景图：
> >
> > 1. **视角**：
> >    - SIFT拼接保留了原始相机的**拍摄视角**，将多张图像通过特征点匹配进行几何对齐，拼接成一个**全景图**。
> >    - 拼接结果依然是从拍摄点出发的视角，仅通过数学变换来实现不同图像的空间一致性。
> > 2. **结果**：
> >    - 输出的是一个**图像全景图**，即多张图像在同一平面上的拼接。
> >    - 无法改变原始拍摄的投影关系，因此与真实场景的几何深度关系较弱，主要用于展示完整的视觉范围。
> >
> > ------
> >
> > ### **两者的核心区别**：
> >
> > | **特性**         | **BEVFormer**                            | **SIFT拼接**                       |
> > | ---------------- | ---------------------------------------- | ---------------------------------- |
> > | **生成视角**     | 上帝视角（鸟瞰视角），通过多视角融合生成 | 原始拍摄视角，保留拍摄点的视角特性 |
> > | **输入要求**     | 需要相机内外参和多视角图像               | 仅需多张图像，无需相机参数         |
> > | **输出结果类型** | 特征图（高语义），不是直接的可视化图像   | 拼接后的全景图（几何对齐图像）     |
> > | **适用场景**     | 自动驾驶、3D感知、语义分割等智能任务     | 图像拼接、全景展示、摄影应用       |
> >
> > ------
> >
> > ### **总结**：
> >
> > - **BEVFormer**专注于从多个拍摄视角推理出**上帝视角的鸟瞰语义特征空间**，用于任务级别的感知（如检测、分割、预测）。
> > - **SIFT拼接**是原始图像在同一视角的几何对齐和范围扩展，生成的仍然是**从拍摄点观察到的全景图**。
> >
> > 两者的视角和用途完全不同，一个用于语义理解，另一个用于视觉展示。
>



# 二、网络结构

![image-20241211150330656](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241211150330656.png)

> **Backbone**
>
> <img src="C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221155123715.png" alt="image-20241221155123715" style="zoom:50%;" />
>
> 基本上啥Backbone都可以，但是最好速度快一些
> ==分别得到6个视角下的特征图，给后续空间注意力用，BEV特征空间中个每一个点都要在这些特征中采样==

> **时间注意力模块**
>
> <img src="C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221155743296.png" alt="image-20241221155743296" style="zoom: 50%;" /><img src="C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221155754875.png" alt="image-20241221155754875" style="zoom: 80%;" />
>
> 类似RNN的方式来利用前面一些时刻的特征
> 在时间上，不同帧中的车和周围物体都会有偏移，如何从历史中选需要的呢？
> 其实依旧是==DeformableAttention==

> **空间注意力模块**
>
> ==要融合多个视角的特征相当于quer y会遍历所有视角找有用的信息==
> 3D空间的点要投影到2D空间，但是由于遮挡或者相机内外参不准
> 那么投影点就是一个参考而已，在这个基础上附近再进行特征采样
> ==其实依旧是DeformableAttention的思想==
> （Local > point >global）
>
> <img src="C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221160051383.png" alt="image-20241221160051383" style="zoom:67%;" />

> **升级优化**
>
> 多尺度特征图
> Corner Pool(针对检测)
> 突出角点特征
> Backbone更强
>
> <img src="C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221163234816.png" alt="image-20241221163234816" style="zoom:67%;" />
>
> 偏移量的预测可以用更大的卷积核1->3
> 多种检测器都可以用，然后做集成
>
> <img src="C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221163322166.png" alt="image-20241221163322166" style="zoom:50%;" />
>
> ![image-20241221163329965](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221163329965.png)

> ![image-20241221163418004](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241221163418004.png)
>
> ==先时间再空间，因为前面时刻BEV有很多信息==
> 相当于要==充分利用先验知识，在此基础上再继续融合当前帧的特征，从而构建当前BEV==
> 重复多次（6次）得到最后的BEV空间特征