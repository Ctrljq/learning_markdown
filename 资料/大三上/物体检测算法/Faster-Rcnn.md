![image-20241112103029776](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20241112103029776.png)



![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/64ce08be6eb7e00429fdd07d43e87199.png)

[论文：faster-rcnn](https://arxiv.org/pdf/1506.01497)

[一文读懂Faster RCNN - 知乎](https://zhuanlan.zhihu.com/p/31426458)

# 一、Covn Layers

Conv layers包含了conv，pooling，relu三种层。

以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：

1. ==所有的Conv层都是：kernel_size=3，pad=1，stride=1==（可以保证处理前后特征图尺度不变）
2. ==所有的pooling层都是：kernel_size=2，pad=0，stride=2==（保证处理前后，`w，h`都缩小为原来的一半）

> ![w +2p - f/ s + 1](https://i-blog.csdnimg.cn/blog_migrate/6991ec1c163d9430852bede94eadda02.png)

为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3： 

![img](https://picx.zhimg.com/v2-3c772e9ed555eb86a97ef9c08bf563c9_1440w.jpg)

图3 卷积示意图

类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。

那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的feature map中都可以和原图对应起来。

# 二、RPN

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e3e5776a2de2720b210036be0f829df1.png)

## 1、整体结构

这张图展示了一个**RPN（Region Proposal Network）** 的网络结构。RPN是一种生成候选区域的方法，广泛用于目标检测任务中，比如Faster R-CNN。下面我将逐步解释每个模块的作用：

### 1. 1x1 卷积层

RPN网络结构图中包含两个1x1卷积层，分别输出18通道和36通道的特征。这里的1x1卷积层的作用主要是降维或升维，用于生成不同的输出。

> #### ==第一条分支（上方的1x1卷积）- 用于分类==
>
> - **18通道的1x1卷积**：上方的1x1卷积层输出18个通道。
>   - 这些通道通常用于**分类任务**，即预测anchor（预设的候选区域）是否包含目标。
>   - 其中的18一般是根据anchor的数量和类别数决定的，假设==每个位置有9个anchor==（常见的设定），每个anchor有两类（前景和背景），那么==需要9×2=18个通道来表示各个anchor的分类得分。==
> - ==**作用**：1x1卷积的作用是将输入特征图转换成所需的输出通道数（在这里是18），以适应分类任务。==
>
> #### ==第二条分支（下方的1x1卷积）- 用于边框回归==
>
> - **36通道的1x1卷积**：下方的1x1卷积层输出36个通道。
>   - 这些通道通常用于**回归任务**，即预测每个anchor的精确边界框位置。
>   - 具体来说，==36通道的输出表示9个anchor的边框回归值（每个anchor有4个参数：\[(dx, dy, dw, dh)\]，共9×4=36）。==
> - **作用**：==同样地，1x1卷积将特征图的通道数转换成所需的输出通道数（在这里是36），以适应回归任务。==
>

### 2. Reshape 层
**作用**：在得到分类和回归的输出后，分别使用Reshape层对特征图进行变换，以便后续处理。

==**Reshape层是技术细节问题，对feature map进行维度变换，使得有一个单独的维度为2，方便在该维度上进行softmax操作，之后再Reshape恢复原状。**==

### 3. Softmax 层
- **作用**：==对分类结果（18通道）进行Softmax处理，将每个anchor的前景和背景的分类得分标准化为概率形式。==
- **目的**：确定每个anchor的目标概率，以便根据概率高低筛选出候选框。

### 4. 🦄Proposal 层

- **作用**：根据Softmax分类结果和边框回归结果生成最终的候选框。
  
  - Proposal层会将==所有anchor的分类得分==（经过Softmax的前景概率）排序，并==结合回归的边框参数==生成更精确的候选框。
  - 这一层会==结合图像信息（`im_info`）==和RPN的输出，==生成多个有较高目标概率的候选框。==
  - Proposal层还可以用非极大值抑制（NMS）来去除重叠框，最终保留一定数量的候选框。
  - > ==Proposal层，这是RPN里最后一个步骤，输入有三个：==
    >
    > - cls 层生成的 ( M / 16 ) x ( N / 16 ) x 2k向量
    > - reg 层生成的 ( M / 16 ) x ( N / 16 ) x 4k 向量（ [ tx , ty , tw , th ] 共 4 个所以是4 k 。注意，这里输出的是坐标偏移量，不是坐标本身，要得到修正后的 anchor 还要用原坐标和这个偏移量运算一下才行。）
    > - im_info=[M, N,scale_factor]
  
  > （1）利用 reg 层的偏移量，对所有的原始 anchor 进行修正
  > （2）利用 cls 层的 scores，按 positive socres由大到小排列所有 anchors ，取前 topN（比如6000个）anchors
  > （3）边界处理，把超出图像边界的 positive anchor 超出的部分收拢到图像边界处，防止后续 RoI pooling 时proposals 超出边界。
  > （4）剔除尺寸非常小的 positive anchor
  > （5）对剩余的 positive anchors 进行NMS（非极大抑制）
  > （6）最后输出一堆 proposals 左上角和右下角坐标值（ [x1,y1,x2,y2] 对应原图 M x N 尺度）
- **输出**：==生成的候选框坐标和得分，这些框会被传递到后续的检测网络进行进一步的处理。==

**整体流程总结**：

1. 输入特征图经过 3x3 卷积，提取局部特征。
2. 通过两个 1x1 卷积分支，生成分类和回归结果。
3. 对分类分支进行 Reshape 和 Softmax 处理，生成前景概率。
4. Proposal 层结合前景概率和回归边框生成候选框。
5. ==这些候选框会传入到后续的 ROI Pooling 和检测网络进行进一步处理。==

- **高效性**：通过 1x1 卷积和 Proposal 层的设计， RPN 能快速生成高质量的候选框。
- **参数少**：1x1 卷积减少了网络的参数量，保持轻量化的同时提高了候选框生成的速度。
- **精度高**：RPN 结合了分类和回归任务，使候选框的位置预测更精确。

以上就是RPN网络结构图的详细解读。这个结构为后续的目标检测模块提供了一个快速、精确的候选框生成机制。

## 2、softmax

> 好的，我们来深入解释一下**为什么需要对18通道的分类结果进行Softmax处理**，以及**Softmax处理的具体作用**。
>
> 在这个RPN（Region Proposal Network）中，每个位置上有多个**anchor**，每个anchor都有两个可能的类别——**前景**（包含目标）和**背景**（不包含目标）。Softmax操作用于将每个anchor的前景和背景分类得分标准化为概率形式，从而帮助模型判断该anchor是否包含目标。
>
> ### 1. RPN的anchor分类输出
>
> 假设我们在特征图的每个位置上生成了9个anchor，那么在分类分支中，会得到一个形状为 \($H \times W \times 18$\) 的输出特征图（其中 \($H$\) 和 \($W$\) 分别是特征图的高度和宽度）。这个输出特征图的通道数为18，表示为每个位置上的9个anchor的分类得分。
>
> 这里的18通道可以被理解为**9个anchor的前景和背景的得分**，即：
> - 每个anchor有两个通道，一个用于表示前景的得分，一个用于表示背景的得分。
> - 9个anchor × 2 个类别（前景和背景）= 18个通道。
>
> ### 2. Softmax处理的作用
>
> ==**Softmax的目的是将得分转化为概率**==，以便于解读。具体来说，对于每个anchor的两个得分（前景和背景），我们可以通过Softmax来标准化它们，使得：
>
> - 两个类别的得分之和为1。
> - 得分被转换为概率的形式，值域在[0, 1]之间。
>
> ### 3. Softmax的计算过程（以单个anchor为例）
>
> 假设某个anchor的前景得分是 `s1`，背景得分是 `s0`，那么通过Softmax可以将这两个得分转换成前景和背景的概率：
>
> $
> p_{\text{前景}} = \frac{\exp(s1)}{\exp(s0) + \exp(s1)}
> $
>
> $p_{\text{背景}} = \frac{\exp(s0)}{\exp(s0) + \exp(s1)}
> $
>
> 其中：
> - \($p_{\text{前景}}$\) 表示该anchor为前景（包含目标）的概率。
> - \($p_{\text{背景}}$\) 表示该anchor为背景（不包含目标）的概率。
>
> ### 4. Softmax处理后的结果解释
>
> 经过Softmax处理后，我们得到每个anchor属于前景和背景的概率。例如：
> - 如果某个anchor的 \($p_{\text{前景}} = 0.8$\)，则表示它包含目标的可能性为80%。
> - 如果某个anchor的 \($p_{\text{前景}} = 0.1$\)，则表示它很可能是背景，不包含目标。
>
> 这种概率化的输出可以帮助RPN网络选择出那些更可能包含目标的anchor。通常会根据**前景的概率大小**，选出前景概率高的anchor作为候选框（proposals），并将它们传递到后续的检测网络中进行进一步的处理。
>
> ### 总结
>
> 1. 18通道的输出表示的是每个anchor的前景和背景得分（9个anchor，每个anchor两个通道）。
> 2. Softmax将每个anchor的前景和背景得分转化为概率，标准化到0-1之间，使得每个anchor属于前景或背景的概率和为1。
> 3. ==RPN根据这些概率值来判断每个anchor是否可能包含目标，从而选择出合适的候选框。==
>
> 这个Softmax操作对于实现目标检测中的候选框筛选至关重要，因为它将分类得分转换成了可以直观解读的概率形式。

# 三、RoI Pooling

## 1、整体流程

**RoI（Region of Interest）Pooling** 是一种用于目标检测网络中的操作，==用于将候选区域（即感兴趣区域，RoIs）统一到固定尺寸，以便与分类器或检测器网络兼容。==RoI Pooling在早期的目标检测网络（如Fast R-CNN）中广泛使用，它解决了CNN中**候选区域大小不一致**的问题。

以下是RoI Pooling的原理和详细解释。

### 1. RoI Pooling的背景
在目标检测任务中，每个图像中可能有多个不同尺寸的候选框（即RoIs），这些候选框代表可能包含目标的区域。然而，CNN中的分类器通常需要一个固定大小的输入。RoI Pooling的作用是将不同尺寸的候选区域**转换成固定大小的特征图**，以便后续的网络处理。

### 2. RoI Pooling的原理

RoI Pooling 的基本操作过程如下：

1. **输入特征图和RoI**：首先，==RoI Pooling接受两个输入：==

   - 从==卷积网络中提取的特征图==（一般是经过多层卷积后的特征图）。
   - ==候选区域（RoI）的坐标==。==这个坐标通常是在原图上的绝对坐标，需要缩放到特征图的尺度上。==

2. **将RoI划分为子窗口**：将每个RoI分割成**固定数量的网格**（例如 \($7 \times 7$\) 或 \($14 \times 14$\) 的小单元）。每个小单元的大小取决于RoI的大小和目标输出的尺寸。比如，如果RoI大小是 \($28 \times 28$\)，且目标输出尺寸是 \($7 \times 7$\)，则每个单元格为 \($4 \times 4$\) 像素。

3. **Max Pooling操作**：在每个小单元格中，执行一个最大池化操作，得到一个单值。这意味着，对于每个子窗口，取出其中的最大值，作为该单元的池化结果。

4. **输出固定大小特征图**：将所有子窗口的池化结果组合起来，就得到了一个固定大小（例如 \($7 \times 7$\)）的特征图。这种特征图可以直接送入全连接层进行分类和回归。

5. > 经过RoI Pooling操作后，每个RoI的特征被转换为固定大小的特征图。这个固定大小的特征图接着会被展平成一个向量，送入全连接层进行分类和回归。
   >
   > - **分类**：判断该RoI属于哪一类目标（或者背景）。
   > - **回归**：计算该RoI的边界框的细化（offsets），用以精确调整候选框的位置和大小。
   >
   > 每个RoI的特征图被展平成一个向量，分别用于**分类**和**边界框回归**。

### 3. 具体示例

假设我们有一个==输入特征图大小为 \($32 \times 32$\)==，其中一个==RoI在原图上的坐标是 ($x1, y1, x2, y2$)，在特征图中对应的区域大小为 \($16 \times 16$\)==。如果==目标输出大小是 \($4 \times 4$\)==，则每个子窗口的大小为 \($4 \times 4$\)：

- 将 \($16 \times 16$\) 的区域划分为 \($4 \times 4$\) 的小单元，每个单元大小为 \($4 \times 4$\) 像素。
- 对每个 \($4 \times 4$\) 单元执行最大池化操作，得到一个最大值。
- 最终得到一个 \($4 \times 4$\) 的输出特征图。

### 4. RoI Pooling的优点

- **固定输出大小**：RoI Pooling将不同尺寸的候选区域（RoIs）转换成固定大小的特征图，方便分类器处理。
- **减少计算量**：通过对每个RoI执行池化操作，可以有效降低后续网络的计算量。
- **适用于变长输入**：即使原始输入图像大小或RoI大小不同，经过RoI Pooling后特征图大小固定，因此适用于多尺寸输入图像。

### 5. RoI Pooling的局限性

RoI Pooling的主要局限在于它的**量化操作**，即将RoI划分为固定网格的过程可能导致信息丢失，特别是对目标边界不敏感。这也是后来提出 **RoI Align** 的原因，RoI Align消除了量化误差，保留了更精确的特征信息。

**总结:**

RoI Pooling 是一种将不规则大小的RoI特征图转换为固定大小特征图的操作。其主要过程包括划分子窗口、执行最大池化、生成固定大小特征图，解决了检测网络中不规则RoI尺寸的问题。

## 2、实例分析

如果有**多个RoI**，输入给 **RoI Pooling** 的是一个卷积特征图，以及多个RoI的坐标列表。这种情况下，RoI Pooling会逐个处理这些RoI，将每个RoI区域池化成固定大小的特征图，最后输出**一个包含所有RoI特征的集合**。具体过程如下：

### 1. 输入给 RoI Pooling 的内容
RoI Pooling的输入包括：
   - **卷积特征图**：通常是通过卷积网络（如ResNet、VGG等）对整张图像进行处理后得到的特征图。例如，如果==原始图像是 \($512 \times 512$\)==，==经过多层卷积和池化，可能得到一个 \($32 \times 32 \times 256$\) 的特征图==。
   - **RoI列表**：这是从RPN（Region Proposal Network）或其他候选框生成方法中得到的候选区域，每个RoI由一组坐标表示。==这些坐标通常是该RoI在原始图像上的位置，需要按比例缩放到特征图上的位置。==

### 2.🦄 RoI Pooling的逐个处理过程

对于每一个RoI，RoI Pooling会执行以下操作：

1. **确定RoI区域在特征图中的位置**：根据原图上的RoI坐标，==按比例将RoI映射到特征图上的坐标。==例如，如果特征图是原图的1/16，那么一个在原图上大小为 \($160 \times 160$\) 的RoI会对应到特征图上的 \($10 \times 10$\) 区域。(感受野映射)

2. **将RoI划分为固定大小的网格**：将==映射到特征图上的RoI区域划分成指定数量的子窗口==（例如 \($7 \times 7$\) 或 \($14 \times 14$\) 的网格），其中每个小单元大小可能不相等，因为RoI可能是不同尺寸。

3. **对每个子窗口执行池化**：通常==使用最大池化，提取每个子窗口的最大值，生成固定大小的特征图==（例如 \($7 \times 7$\)）。这样，不管RoI原本的尺寸如何，池化结果都会得到相同大小的特征图。

4. **保存池化结果**：==每个RoI的池化结果（固定大小的特征图）都会被保存下来，构成一个特征集合。==

### 3. RoI Pooling的输出
如果有**多个RoI**，RoI Pooling的输出将是一个**包含所有RoI特征图的张量**。假设有 `N` 个RoI，且每个RoI经过RoI Pooling后生成 \($C \times H \times W$\) 大小的特征图，则输出的张量维度是 \($N \times C \times H \times W$\)，即包含 `N` 个固定大小的特征图。

### 4. 后续的处理
==在Fast R-CNN等网络结构中，这个包含 `N` 个RoI特征图的张量会被送入后续的全连接层进行分类和边界框回归。==具体来说：

   - **分类器**会对每个RoI的特征进行分类，判断该RoI属于哪一类目标（或背景）。
   - **回归层**会根据每个RoI的特征，计算该RoI的边界框调整值（offsets），用于进一步微调候选框的位置和大小。

### 示例流程总结
假设有3个RoI，每个RoI池化成 \($7 \times 7 \times 256$\) 的特征图，则最终RoI Pooling输出的形状是 \($3 \times 256 \times 7 \times 7$\)。这个张量包含了3个RoI的特征，送入网络的后续层进行独立的分类和回归。

### 关键点总结

- ==**RoI Pooling逐个处理每个RoI**，对每个RoI区域进行固定大小池化。==
- ==**RoI Pooling的输出是一个包含所有RoI特征的张量**，其形状为 \($N \times C \times H \times W$\)。==
- ==每个RoI的池化特征图大小相同，可以进一步送入全连接层进行分类和边界框回归处理。==

# 四、Classification

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/815a221accab51aecd116240d01dbdcd.png)

注意这里的分类和RPN中的分类不同，RPN中只是二分类，区分目标还是背景；这里的分类是要对之前的所有positive anchors识别其具体属于哪一类。

## 1、具体流程

RoI Pooling层的输出是一个形状为 \($N \times C \times 7 \times 7$\) 的张量（假设每个RoI池化后大小为 \($7 \times 7$\)），其中：
- \(N\) 是RoI的数量。
- \(C\) 是通道数（卷积特征图的深度）。
- \($7 \times 7$\) 是每个RoI的固定大小特征图。

接下来，对这个输出张量进行分类的过程通常包含以下几个步骤：

### 1. 展平每个RoI特征图
- 首先，将每个RoI的特征图展平成一个向量。
- 如果特征图的大小为 \($7 \times 7 \times C$\)，展平后的向量大小就是 \($7 \times 7 \times C$\)。
- 展平的目的是将固定大小的特征图转换为一维向量，以便输入到全连接层。

假设 \(C = 256\)，那么展平后的大小为 \($7 \times 7 \times 256 = 12544$\) 维。

### 2. 输入全连接层（Fully Connected Layer）
- 将展平后的特征向量输入到一个或多个全连接层。
- ==全连接层可以进一步提取特征并降低维度，将输入转换为用于分类和边界框回归的向量。==
- 经过全连接层后，每个RoI的特征向量会变成较小的向量，通常包含两个分支，分别用于分类和边界框回归。

### 3. 分类分支
- 分类分支用于判断每个RoI的类别。
- 最后一层通常是一个全连接层，其输出维度等于类别数量加上背景类。例如，如果有20个物体类别，加上背景类，共21个类别，分类分支的输出维度就是21。
- **Softmax激活函数**会应用于最后的输出层，将每个RoI的预测结果转化为概率分布，从而判断该RoI属于哪一类目标（或背景）。

#### 示例：假设共有20个类别
- 输出维度为21（20类目标 + 1类背景）。
- 输出形状为 \($N \times 21$\)，其中每个RoI对应一个包含21个概率的向量。

### 4. 边界框回归分支
- 边界框回归分支用于调整候选框的精确位置和大小。
- 回归分支输出的维度通常是 \($4 \times K$\)，其中 \(K\) 是类别数量，4表示边界框的4个参数（中心坐标的偏移量 \(($\Delta x, \Delta y$)\) 和宽高缩放 \($(\Delta w, \Delta h)$\)）。
- 每个RoI会根据其类别使用相应的回归参数，得到最终的边界框预测。

#### 示例：如果有20个类别

- 回归分支的输出维度是 \($N \times 80$\)（即 \($4 \times 20$\)）。
- 每个RoI对应一个包含80个元素的向量，表示所有类别的边界框调整参数。

### 5. 输出和后处理
- 对于每个RoI，最终会得到一个分类预测和一个边界框回归结果。
- 后处理通常包括：
  - **非极大值抑制（NMS）**：去除重叠的预测框，保留置信度最高的框。
  - **置信度筛选**：剔除低置信度的预测。

### 总结
整个分类过程如下：
1. **RoI特征展平**，转为一维向量。
2. **全连接层提取特征**，输出两个分支。
3. **分类分支**使用Softmax获得类别概率。
4. **回归分支**获得边界框调整参数。
5. **后处理**，如NMS，得到最终分类和位置结果。

# 五、训练

Faster RCNN由于是two-stage检测器，训练要分为两个部分进行，一个是训练RPN网络，一个是训练后面的分类网络。为了清晰描述整个训练过程，首先明确如下两个事实：

RPN网络 = 特征提取conv层（下面简称共享conv层） + RPN特有层（3x3卷积、1x1卷积等）
Faster RCNN网络 = 共享conv层 + Faster RCNN特有层（全连接层）

## 1、RPN

RPN网络训练有两个Loss：

- Lcls：softmax loss，用于分类anchors属于前景还是背景（也有说用二分类交叉熵Loss的）
- Lreg：smooth L1 loss，用于修正anchor框，前面乘了一个pi*表示只回归有目标的框

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7026a67cd71b9d8b5094b92ca729de83.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/65aefe59485fd2ab9802bfa07fc7c64c.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7dca0985aea05c3a2bf9a7fc00b11648.png)