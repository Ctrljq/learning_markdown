# yolo-swintransformer

```
设计并优化了基于YOLO系列的物体检测算法，特别针对小目标检测效果进行了改进。采用SwinTransforer架构优化了模型的backbone模块，在提升检测速度的同时，提高了小目标检测精度。
```

用swintransformer结构替代了yolov5原生的backbone模块。（backbone和head都用了各自预训练权重）

head输出多尺度特征融合。【矩形训练】

为了不影响实时检测的效率，并提高检测准确率，选择了引入transformer注意力机制，但传统的transformer最大的问题之一就是计算量太大，全局计算注意力占用太多计算资源，所以选择使用swintransformer。用窗口和分层的形式来替代长序列。由于窗口内部进行self attn计算。

==这种机制使得模型能够聚焦于局部区域的细节==。==对小目标的边界和纹理信息有更精确的捕获能力==。

并且多尺度特征结合 FPN（Feature Pyramid Network）等技术，进一步增强小目标检测效果。

> 数据集层面：就是仿照coco数据集格式处理的，边界框（Bounding Box），类别（Class）.
>
> 使用了脚本，结合了数据增强的操作

# slowfast-efficientvit

```
针对施工现场的作业行为，设计并实现了基于S1owFast的行为识别模型。引入了注意力机制，使得模型能更准确地关注施工人员佩戴的装备特征，从而提高行为识别的精度和鲁棒性。
```

```
设计了高效的2D特征提取方法(EfficientViT)，有效提取施工现场关键特征，提升了模型的精度。
```

EfficientViT用了预训练权重，但slowfast没有（因为）

**Slow Path** 专注于低频变化，适合捕捉空间特征（如物体的位置、形状等），通过**空间注意力**进一步强调帧内重要区域。然后引入efficientvit来做这个空间注意力模块。

**Fast Path** 专注于高频变化，适合捕捉时间动态（如快速动作或短时运动），通过**时间注意力**动态聚焦重要时间点。但考虑到实时处理的特性，再引入transformer可能计算量会大幅提高，就暂时没做。

> **体育视频分析**：Fast Path 提取快速动作（如挥拍或跳跃），Slow Path 聚焦动作细节（如身体姿态）。
>
> **异常检测**：Fast Path 定位时间异常（如突然动作），Slow Path 确定空间异常（如异常区域）。
>
> **视频场景理解**：Fast Path 捕捉时间动态场景变化，Slow Path 聚焦空间结构和物体特征。



> 数据集标注：视频编号，当前时刻，X1,Y1,X2,Y2，行为，personid
>
> 使用先用脚本对视频切帧，再用LabelImg标注工具拉框，扩展标签格式记录行为id和personid，最后使用转换脚本得到正确的ava标注格式的csv文件
>
> <img src="C:/Users/HUAWEI/Documents/Tencent%20Files/1436941594/nt_qq/nt_data/Pic/2024-12/Ori/d5229f86cda5f362b095041e5318537f.png" alt="d5229f86cda5f362b095041e5318537f" style="zoom:50%;" />

# openpose

```
基于OpenPose框架，优化了人体姿态估计模型，结合骨骼点连接状态信息，有效提升了在视频中出现蔽现象(如遮
挡)时的识别效果。
```

就是facebook原生的，权重也是原生的。这个网络没有明显的backbone与head的区分。是多监督任务，多个输出阶段都要算损失。就是一个简单的vgg结构。

感觉这个算法特点就是如果要自己训练就要自己处理一下标签。将标签的点映射到特征图尺寸上（并且是heatmap的形式），还有连接方式也需要自己制作。

但做得有点粗糙还需要优化。DeepSort是一个基于卡尔曼滤波和ReID的跟踪算法【目标检测+追踪||对检测到底bbox提取各项特征后匹配】，能够根据目标的外观特征和运动轨迹为每个目标分配稳定的ID。

# Bevformer

```
基于BEVForet框架，构建了施工现场的3D场景特征空间，并应用于物体检测任务，增强了模型对复杂场景的
感知能力。
```

因为在读到bevformer这篇论文时，发现论文末最后几幅可视化的图显示了，如果使用bev特征图直接输出到检测头进行物体检测，对遮挡的目标仍然有比较好的检测结果。所以这个是物体检测的一个优化版本。

# 报警判断

```
结合时序相关特征和BEV特征空间，设计了违章识别判断规则，实现了对施工现场违章行为的自动识别与报警功
能。
```

openpose单帧检测，连续帧（重心匈牙利匹配【==不合理，重合时有问题==】）检测结合行为识别共同判断。

# 可改进的地方

## deepsort跟踪

需要设计任务管道，保证任务A的输出及时传递给任务B。

## 物体检测anchor free（更利于小目标），detr思想

> 在使用 **DETR（DEtection TRansformer）** 时，由于其是 **Anchor-Free** 的直接端到端物体检测框架，输出固定数量（通常为100）的预测框，每个框由模型直接回归。为了判断哪些框应该被舍弃，DETR引入了以下机制：
>
> ------
>
> ### 1. **匈牙利匹配（Hungarian Matching）**
>
> 在==训练阶段==，DETR通过**匈牙利匹配算法**来匹配预测框和真实框：
>
> - **目标**：最小化预测框和真实框之间的匹配损失（包含类别损失和边界框损失）。
> - 匹配后，未匹配到真实框的预测框被视为背景框。
> - 匹配过程会强制模型为每个物体预测一个高质量框，同时生成合理的背景预测。
>
> ------
>
> ### 2. **No Object 类别（背景类）**
>
> DETR的分类输出中引入了一个特殊类别：**No Object（背景类）**，用于标识预测框是否为空（即不包含有效物体）。
>
> - 输出的每个预测框都有一个类别概率分布，其中背景类别概率较高的框会被视为无效框。
> - 训练时，通过匹配的匈牙利算法，部分框会被分配为背景类。
>
> ------
>
> ### 3. **置信度阈值**
>
> 在==推理阶段==，利用框的分类分数（置信度）判断有效性：
>
> - 设置一个分类分数阈值（例如`0.5`）。
> - 当某个预测框的类别置信度低于此阈值时，该框会被舍弃。
>
> ------
>
> ### 4. **去重与NMS（非极大值抑制）**
>
> 默认DETR不使用NMS（非极大值抑制），但预测框可以通过分类分数的排序自然筛选出高质量的框。若进一步需要去掉重复框：
>
> - 可手动添加**Soft-NMS** 或 **普通NMS** 来去除重叠较大的框。
>
> ------
>
> ### 5. **未匹配框的自动舍弃机制**
>
> 在DETR模型中，由于预测了固定数量的框，未能与真实框匹配的部分框会自动被学习为背景框。这通过损失函数中的背景类别项体现，具体如下：
>
> - 分类损失（交叉熵）：背景类的高置信度会降低分类损失。
> - 回归损失（L1损失和GIoU）：无匹配框的回归损失对总损失的影响很小，因此模型会优先优化真实框的预测质量。
>
> ------
>
> ### 工作流程总结
>
> 1. **训练阶段**：
>    - ==每个预测框要么匹配真实框（通过匈牙利算法），要么被视为背景框。==
>    - ==未匹配到真实框的预测框会被强制学习为背景类别。==
> 2. **推理阶段**：
>    - 模型输出100个预测框。
>    - ==利用置信度阈值过滤低分框。==
>    - 根据实际需求，可再引入NMS去除冗余框。
>
> ------
>
> ### 示例应用
>
> 1. **标准物体检测**： 仅使用置信度过滤无效框。
> 2. **目标稀疏场景**： 如果场景中物体稀疏，可能需要设置较高的置信度阈值。
> 3. **多目标检测**： 若物体密集，可结合置信度和IoU过滤，确保检测框的独立性。
>
> 这种==直接预测固定数量框的机制避免了传统Anchor设计的复杂性，同时通过端到端的损失优化模型性能。==

## SAM2（SAM2 可以分割出工人的精确轮廓或身体部位（如手、头部、躯干），为 SlowFast 提供更干净的输入特征。）